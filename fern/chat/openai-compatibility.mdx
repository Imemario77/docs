---
title: OpenAI compatibility
subtitle: Seamlessly migrate existing OpenAI integrations to Vapi with zero code changes
---

## Overview

Migrate your existing OpenAI chat applications to Vapi without changing a single line of code. Perfect for teams already using OpenAI SDKs, third-party tools expecting OpenAI API format, or developers who want to leverage existing OpenAI workflows.

**What You'll Build:**
* Drop-in replacement for OpenAI chat endpoints using Vapi assistants
* Migration path from OpenAI to Vapi with existing codebases
* Integration with popular frameworks like LangChain and Vercel AI SDK
* Production-ready server implementations with both streaming and non-streaming

**Migration Benefits:**
* Keep existing OpenAI SDK code unchanged
* Leverage Vapi's advanced assistant capabilities
* Maintain familiar developer experience
* Easy rollback if needed

## Prerequisites

* Completed [Chat quickstart](/docs/chat/quickstart) tutorial
* Existing OpenAI integration or familiarity with OpenAI SDK
* Node.js/Python environment for testing migrations

## Scenario

We'll migrate "TechFlow's" existing OpenAI-powered customer support chat to use Vapi assistants, maintaining all existing functionality while gaining access to Vapi's advanced features like custom voices and tools.

---

## 1. Quick Migration Test

<Steps>
  <Step title="Install the OpenAI SDK">
    If you don't already have it, install the OpenAI SDK:
    
    ```bash title="Install OpenAI SDK"
    npm install openai
    ```
  </Step>
  <Step title="Test with OpenAI-compatible endpoint">
    Use your existing OpenAI code with minimal changes:
    
    ```bash title="Test OpenAI Compatibility"
    curl -X POST https://api.vapi.ai/chat/responses \
      -H "Authorization: Bearer YOUR_VAPI_API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "gpt-4o",
        "input": "Hello, I need help with my account",
        "stream": false,
        "assistantId": "your-assistant-id"
      }'
    ```
  </Step>
  <Step title="Verify response format">
    The response follows OpenAI's structure with Vapi enhancements:
    
    ```json title="OpenAI-Compatible Response"
    {
      "id": "response_abc123",
      "object": "chat.response",
      "created": 1642678392,
      "model": "gpt-4o",
      "output": [
        {
          "role": "assistant",
          "content": [
            {
              "type": "text",
              "text": "Hello! I'd be happy to help with your account. What specific issue are you experiencing?"
            }
          ]
        }
      ],
      "usage": {
        "prompt_tokens": 12,
        "completion_tokens": 23,
        "total_tokens": 35
      }
    }
    ```
  </Step>
</Steps>

---

## 2. Migrate Existing OpenAI Code

<Steps>
  <Step title="Update your OpenAI client configuration">
    Change only the base URL and API key in your existing code:
    
    ```javascript title="Before (OpenAI)"
    import OpenAI from 'openai';
    
    const openai = new OpenAI({
      apiKey: 'sk-...' // OpenAI API key
    });
    ```
    
    ```javascript title="After (Vapi)"
    import OpenAI from 'openai';
    
    const openai = new OpenAI({
      apiKey: 'YOUR_VAPI_API_KEY', // Vapi API key
      baseURL: 'https://api.vapi.ai/chat'
    });
    ```
  </Step>
  <Step title="Update your function calls">
    Change `chat.completions.create` to `responses.create` and add `assistantId`:
    
    ```javascript title="Before (OpenAI Chat Completions)"
    const response = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'user', content: 'What is the capital of France?' }
      ],
      stream: false
    });
    
    console.log(response.choices[0].message.content);
    ```
    
    ```javascript title="After (Vapi Compatibility)"
    const response = await openai.responses.create({
      model: 'gpt-4o',
      input: 'What is the capital of France?',
      stream: false,
      assistantId: 'your-assistant-id'
    });
    
    console.log(response.output[0].content[0].text);
    ```
  </Step>
  <Step title="Test your migrated code">
    Run your updated code to verify the migration works:
    
    ```javascript title="migration-test.js"
    import OpenAI from 'openai';
    
    const openai = new OpenAI({
      apiKey: 'YOUR_VAPI_API_KEY',
      baseURL: 'https://api.vapi.ai/chat'
    });
    
    async function testMigration() {
      try {
        const response = await openai.responses.create({
          model: 'gpt-4o',
          input: 'Hello, can you help me troubleshoot an API issue?',
          stream: false,
          assistantId: 'your-assistant-id'
        });
        
        console.log('Migration successful!');
        console.log('Response:', response.output[0].content[0].text);
      } catch (error) {
        console.error('Migration test failed:', error);
      }
    }
    
    testMigration();
    ```
  </Step>
</Steps>

---

## 3. Implement Streaming with OpenAI SDK

<Steps>
  <Step title="Migrate streaming chat completions">
    Update your streaming code to use Vapi's streaming format:
    
    ```bash title="Streaming via curl"
    curl -X POST https://api.vapi.ai/chat/responses \
      -H "Authorization: Bearer YOUR_VAPI_API_KEY" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "gpt-4o",
        "input": "Explain how machine learning works in detail",
        "stream": true,
        "assistantId": "your-assistant-id"
      }'
    ```
  </Step>
  <Step title="Update streaming JavaScript code">
    Adapt your existing streaming implementation:
    
    ```javascript title="streaming-migration.js"
    async function streamWithVapi(userInput) {
      const stream = await openai.responses.create({
        model: 'gpt-4o',
        input: userInput,
        stream: true,
        assistantId: 'your-assistant-id'
      });

      let fullResponse = '';
      
      for await (const event of stream) {
        if (event.type === 'response.output_text.delta') {
          process.stdout.write(event.delta);
          fullResponse += event.delta;
        }
      }
      
      console.log('\n\nComplete response received.');
      return fullResponse;
    }

    // Test streaming
    streamWithVapi('Write a detailed explanation of REST APIs');
    ```
  </Step>
  <Step title="Handle conversation context">
    Implement context management using Vapi's approach:
    
    ```javascript title="context-management.js"
    class ContextualChatSession {
      constructor(apiKey, assistantId) {
        this.openai = new OpenAI({
          apiKey: apiKey,
          baseURL: 'https://api.vapi.ai/chat'
        });
        this.assistantId = assistantId;
        this.lastResponseId = null;
      }

      async sendMessage(input, stream = false) {
        const requestParams = {
          model: 'gpt-4o',
          input: input,
          stream: stream,
          assistantId: this.assistantId
        };

        // Add context if we have a previous response
        if (this.lastResponseId) {
          requestParams.previous_response_id = this.lastResponseId;
        }

        const response = await this.openai.responses.create(requestParams);

        if (!stream) {
          this.lastResponseId = response.id;
          return response.output[0].content[0].text;
        }

        return response; // Return stream for streaming requests
      }
    }

    // Usage example
    const session = new ContextualChatSession('YOUR_VAPI_API_KEY', 'your-assistant-id');

    const response1 = await session.sendMessage("My name is Sarah and I'm having login issues");
    console.log('Response 1:', response1);

    const response2 = await session.sendMessage("What was my name again?");
    console.log('Response 2:', response2); // Should remember "Sarah"
    ```
  </Step>
</Steps>

---

## 4. Framework Integrations

<Steps>
  <Step title="Integrate with LangChain">
    Use Vapi with LangChain's OpenAI integration:
    
    ```javascript title="langchain-integration.js"
    import { ChatOpenAI } from "langchain/chat_models/openai";
    import { HumanMessage } from "langchain/schema";

    const chat = new ChatOpenAI({
      openAIApiKey: "YOUR_VAPI_API_KEY",
      configuration: {
        baseURL: "https://api.vapi.ai/chat"
      },
      modelName: "gpt-4o",
      streaming: false
    });

    // Custom method to handle Vapi-specific parameters
    async function chatWithVapi(message, assistantId) {
      // LangChain doesn't directly support assistantId, so we'll use a custom approach
      const response = await fetch('https://api.vapi.ai/chat/responses', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer YOUR_VAPI_API_KEY`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'gpt-4o',
          input: message,
          assistantId: assistantId,
          stream: false
        })
      });

      const data = await response.json();
      return data.output[0].content[0].text;
    }

    // Usage
    const response = await chatWithVapi(
      "What are the best practices for API design?",
      "your-assistant-id"
    );
    console.log(response);
    ```
  </Step>
  <Step title="Integrate with Vercel AI SDK">
    Use Vapi with Vercel's AI SDK:
    
    ```javascript title="vercel-ai-integration.js"
    import { openai } from '@ai-sdk/openai';
    import { generateText, streamText } from 'ai';

    // Create a custom Vapi provider
    const vapiOpenAI = openai({
      apiKey: 'YOUR_VAPI_API_KEY',
      baseURL: 'https://api.vapi.ai/chat'
    });

    // Non-streaming text generation
    async function generateWithVapi(prompt, assistantId) {
      // Note: Vercel AI SDK may need custom implementation for assistantId
      const response = await fetch('https://api.vapi.ai/chat/responses', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer YOUR_VAPI_API_KEY`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'gpt-4o',
          input: prompt,
          assistantId: assistantId,
          stream: false
        })
      });

      const data = await response.json();
      return data.output[0].content[0].text;
    }

    // Streaming implementation
    async function streamWithVapi(prompt, assistantId) {
      const response = await fetch('https://api.vapi.ai/chat/responses', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer YOUR_VAPI_API_KEY`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          model: 'gpt-4o',
          input: prompt,
          assistantId: assistantId,
          stream: true
        })
      });

      const reader = response.body.getReader();
      const decoder = new TextDecoder();

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        // Process SSE events here
        console.log(chunk);
      }
    }

    // Usage examples
    const text = await generateWithVapi(
      "Explain the benefits of microservices architecture",
      "your-assistant-id"
    );
    console.log(text);
    ```
  </Step>
  <Step title="Create a production server">
    Build a server that exposes Vapi through OpenAI-compatible endpoints:
    
    ```javascript title="production-server.js"
    import express from 'express';
    import OpenAI from 'openai';

    const app = express();
    app.use(express.json());

    const openai = new OpenAI({
      apiKey: process.env.VAPI_API_KEY,
      baseURL: 'https://api.vapi.ai/chat'
    });

    // OpenAI-compatible chat completions endpoint
    app.post('/v1/chat/completions', async (req, res) => {
      try {
        const { messages, model, stream = false, assistant_id } = req.body;
        
        if (!assistant_id) {
          return res.status(400).json({ 
            error: 'assistant_id is required for Vapi compatibility' 
          });
        }

        // Convert OpenAI messages format to Vapi input
        const lastMessage = messages[messages.length - 1];
        const input = lastMessage.content;

        if (stream) {
          res.setHeader('Content-Type', 'text/event-stream');
          res.setHeader('Cache-Control', 'no-cache');
          res.setHeader('Connection', 'keep-alive');

          const stream = await openai.responses.create({
            model: model || 'gpt-4o',
            input: input,
            stream: true,
            assistantId: assistant_id
          });

          for await (const event of stream) {
            if (event.type === 'response.output_text.delta') {
              const openaiFormat = {
                id: event.id,
                object: 'chat.completion.chunk',
                created: Math.floor(Date.now() / 1000),
                model: model || 'gpt-4o',
                choices: [{
                  index: 0,
                  delta: { content: event.delta },
                  finish_reason: null
                }]
              };
              res.write(`data: ${JSON.stringify(openaiFormat)}\n\n`);
            }
          }

          res.write('data: [DONE]\n\n');
          res.end();
        } else {
          const response = await openai.responses.create({
            model: model || 'gpt-4o',
            input: input,
            stream: false,
            assistantId: assistant_id
          });

          // Convert to OpenAI format
          const openaiResponse = {
            id: response.id,
            object: 'chat.completion',
            created: Math.floor(Date.now() / 1000),
            model: model || 'gpt-4o',
            choices: [{
              index: 0,
              message: {
                role: 'assistant',
                content: response.output[0].content[0].text
              },
              finish_reason: 'stop'
            }],
            usage: response.usage || {
              prompt_tokens: 0,
              completion_tokens: 0,
              total_tokens: 0
            }
          };

          res.json(openaiResponse);
        }
      } catch (error) {
        console.error('Server error:', error);
        res.status(500).json({ 
          error: { 
            message: error.message,
            type: 'server_error'
          }
        });
      }
    });

    app.listen(3000, () => {
      console.log('Vapi-OpenAI compatibility server running on port 3000');
    });
    ```
  </Step>
</Steps>

---

## 5. Test Your Migration

<Steps>
  <Step title="Test with existing OpenAI clients">
    Use your production server with standard OpenAI clients:
    
    ```bash title="Test with curl (OpenAI format)"
    curl -X POST http://localhost:3000/v1/chat/completions \
      -H "Content-Type: application/json" \
      -d '{
        "model": "gpt-4",
        "messages": [
          {"role": "user", "content": "Hello, I need technical support"}
        ],
        "assistant_id": "your-assistant-id",
        "stream": false
      }'
    ```
  </Step>
  <Step title="Test streaming compatibility">
    Verify streaming works with OpenAI format:
    
    ```javascript title="test-streaming-compatibility.js"
    import OpenAI from 'openai';

    // Test with your local server
    const localOpenAI = new OpenAI({
      apiKey: 'dummy-key', // Not used by local server
      baseURL: 'http://localhost:3000/v1'
    });

    const stream = await localOpenAI.chat.completions.create({
      model: 'gpt-4',
      messages: [
        { role: 'user', content: 'Explain cloud computing in detail' }
      ],
      stream: true,
      assistant_id: 'your-assistant-id'
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      process.stdout.write(content);
    }
    ```
  </Step>
  <Step title="Compare performance and responses">
    Run side-by-side tests to validate migration quality:
    
    ```javascript title="migration-validation.js"
    async function compareResponses(prompt) {
      console.log('Testing prompt:', prompt);
      console.log('---');

      // Test Vapi response
      const vapiResponse = await openai.responses.create({
        model: 'gpt-4o',
        input: prompt,
        assistantId: 'your-assistant-id',
        stream: false
      });

      console.log('Vapi Response:');
      console.log(vapiResponse.output[0].content[0].text);
      console.log('---');

      // You could also test with original OpenAI here for comparison
      // const openaiResponse = await originalOpenAI.chat.completions.create({...});
    }

    // Test various scenarios
    await compareResponses("What are your customer support hours?");
    await compareResponses("I'm having trouble with API authentication");
    await compareResponses("Can you explain your pricing plans?");
    ```
  </Step>
</Steps>

## What You've Built

Congratulations! You now have:

✅ **Seamless OpenAI to Vapi migration** with zero code changes required  
✅ **Production-ready compatibility server** that translates between formats  
✅ **Framework integrations** with LangChain and Vercel AI SDK  
✅ **Streaming and non-streaming support** maintaining OpenAI API patterns  
✅ **Context management** using Vapi's advanced conversation features  
✅ **Validation testing** to ensure migration quality  

## Key Differences from OpenAI

| Aspect | OpenAI | Vapi Compatible |
|--------|--------|-----------------|
| **Endpoint** | `/chat/completions` | `/chat/responses` |
| **Messages** | `messages` array | `input` string + `previous_response_id` |
| **Assistant** | Model determines behavior | `assistantId` required |
| **Context** | Full message history | Efficient context linking |
| **Base URL** | `api.openai.com/v1` | `api.vapi.ai/chat` |

## Migration Checklist

- [ ] Updated API keys and base URLs
- [ ] Changed `chat.completions.create` to `responses.create`
- [ ] Added `assistantId` to all requests
- [ ] Updated context management to use `previous_response_id`
- [ ] Tested both streaming and non-streaming modes
- [ ] Validated response quality matches expectations
- [ ] Updated error handling for Vapi-specific errors

## Next Steps

Enhance your migrated system:

* **[Explore Vapi-specific features](/docs/chat/quickstart)** - Leverage advanced assistant capabilities
* **[Add voice capabilities](/docs/calls/call-outbound)** - Extend beyond text to voice interactions
* **[Integrate tools](/docs/tools/custom-tools)** - Give your assistant access to external APIs
* **[Optimize for streaming](/docs/chat/streaming)** - Improve real-time user experience

<Callout>
Need help? Chat with the team on our [Discord](https://discord.com/invite/pUFNcf2WmH) or mention us on [X/Twitter](https://x.com/Vapi_AI).
</Callout>
